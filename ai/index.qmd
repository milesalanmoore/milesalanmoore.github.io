---
title: "Artificial Intelligence Use"
page-layout: full
toc-location: right
---

I believe in being transparent and explicit about the use of Generative Artificial Intelligence (AI) tools, like Large Language Models (LLMs), in my intellectual work. On this page, you'll find a collection of statements about if and how I use AI for specific tasks.

::: {.fs-3}
> Everything in my research and on this website was written by me.
:::

AI tools are becoming ubiquitous in the daily workflow of many knowledge workers. These text-generation models are showing up in email applications, coding environments, job application portals, internet search engines, and just about anywhere else they can be embedded. The implicit promise of these tools is that they save time and money by reducing the amount of human effort required in tasks ranging from mundane to mission-critical. Here's how I am using (or not using) these tools:

::: {.callout-note}
#### The /ai manifesto

I was inspired to write this page by [Andrew Heiss' own version](https://www.andrewheiss.com/ai/) of this sort of statement, as well as the references he linked: ["The '/ai' manifesto"](https://www.bydamo.la/p/ai-manifesto). See [the /ai public database](https://slashai.page/) for more.
:::

# Writing
I am strongly opposed to the use of AI for *writing* anything. I have been particularly persuaded by arguments in this area that, because LLMs are statistical prediction machines, the model is completely and utterly incapable of regard for the moral or ethical content of the text that it generates. It is also almost impossible to verify the intellectual provenance of any text generated. This means that they generate meaningless, immoral, nonsensical text with no real ideas stringing anything together. In short, there is not a human behind the writing, so how could it possibly be worth reading? For this reason, I do not use LLMs to generate any portion of a research paper, cover letter, email, blog post, course assignment, or similar text. 

I do sometimes prompt an LLM to review my writing for grammatical, logical, and structural errors.

# Coding
I am still figuring out how I feel about this one. I do sometimes use LLMs in my coding—most often to:

::: {}
1. Document or comment on long scripts or functions.  
2. Turn a chunk of code I’ve written into a more general function with type checks and tests.  
3. Translate from one language to another.  
4. Explain complex code chunks.  
5. Review code for logical errors or potential problems.  
6. Plotting data
:::

I've found LLMs are really crazy good at these things and bad at most others. An important note is that I am *extremely* against the use of LLMs/AI for coding (even in the use cases above) when it comes to anything near a statistical model or other research-related data analysis. See the **Researching** section below for more discussion on this.

# Learning
I think this might be one of the worst use cases for LLMs, depending on *what* exactly is being "learned." I value the struggle in learning something new. I think that is what actually develops problem-solving and critical thinking. Admittedly, though, I did use OpenAI's ChatGPT to help me understand some Numerical Linear Algebra homework recently, and it was really helpful. As a general pattern, though, I try not to use it. Whenever I find myself using it I later come to regret not having just gone straight to a textbook, documentation, or other primary source where I realized I would've learned what I needed and then some...

# Researching {#sec:research}
I only use LLMs in my research in the ways described in **Writing** (grammar check, structural critique) and **Coding**.  If a human were to do something that would in turn earn authorship, I believe AI of any kind is strictly off-limits in these cases. Further, as a personal rule, I avoid ever copying and pasting anything but code from an LLM into my own work. Even with code, I strictly *do not* use LLMs to write, diagnose, or interpret statistical models. I do use LLMs to help me generate plots. I've found them to be very nice for just describing the type of plot I want (e.g., "write matplotlib for a dataframe object with columns X, Y, Z, W that represents Z as points in X and Y colored by W").

# Images and Video
I have less sophisticated thoughts here currently. I have used Generative AI to create images before. In fact, the little favicon for this website was generated using an LLM! When I see AI generated figures in manuscripts though, something about it irritates me a bit. Maybe I shouldn't be so grumpy about it though... if the graphic communicates information effectively I guess it is fine.
